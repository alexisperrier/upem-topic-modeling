var data = [{"id":1,"body":"<?xml version='1.0' encoding='UTF 8'?><html>\n<body>Replacing bulky film cameras allowed astronauts to become filmmakers<figure>\n<img src='http://spectrum.ieee.org/image/Mjc0ODIyMw.jpeg'/>\n<figcaption>Photo: NASA</figcaption>\n</figure>\n<div>\n<p>\u201cThere\u2019s no way you can match what you see with your own eyes\u2014but this is close,\u201d says Barry \u201cButch\u201d Wilmore. As part of the crew of the International Space Station, Wilmore was one of the astronauts turned cinematographers who captured footage for the latest IMAX documentary, <em>A Beautiful Planet, </em>which opens today, 29 April.</p>\n<p/>\n<p>\n<em>\n<a shape='rect' href='http://abeautifulplanet.imax.com/'>A Beautiful Planet</a>\n</em> takes full advantage of the vantage point of the ISS and its multi windowed <a shape='rect' href='https://www.nasa.gov/mission_pages/station/structure/elements/cupola.html'>viewing cupola</a>. The mesmerizing glowing veils of the aurorae, the night time splatter of cities across the continents, and swirling storm systems fill the giant IMAX screen in 3 D, leaving a lasting impression of wonder.</p>\n<p/>\n<p>The movie also includes many scenes of life on board the ISS, giving some of the best views to date about what a working day is really like in a cluttered collection of floating boxes. The moviemakers\u2019 goal in juxtaposing the interior shots with those aimed outward toward the blue ball we call home is to reinforce <em>A Beautiful Planet</em>\u2019s environmental theme. The point: Just as considerable effort is required to keep a space station habitable, effort also needs to be put into limiting ecological damage on spaceship Earth so as to keep it habitable too.</p>\n<p/>\n<p>Of course, <em>A Beautiful Planet</em> is not first IMAX movie shot in space. Indeed, <a shape='rect' href='http://www.imdb.com/name/nm0616880/'>Toni Myers</a>, the movie\u2019s director and producer also helmed 2002\u2019s <em>\n<a shape='rect' href='http://spacestation.imax.com/'>Space Station 3D</a>\n</em> and 2010\u2019s <em>\n<a shape='rect' href='http://hubblesite.org/hubble_20/imax_hubble_3d/'>Hubble 3D</a>.</em> But what\u2019s different is that <em>A Beautiful Planet</em> was shot entirely using digital cameras, dramatically changing the nature of the filmmaking process.</p>\n<p/>\n<p>Myers explained that the earlier IMAX cameras and their film packs were so bulky that, \u201cYou got seven minutes of footage on a mission, in 30 second takes. And every take was the first and only take.\u201d In contrast, the digital cameras allowed as many takes as were needed; footage was stored on small external hard drives sent back to Earth with returning astronauts or <a shape='rect' href='http://www.spacex.com/dragon'>Dragon cargo ships</a>.  In addition, relatively low resolution versions could be directly downlinked to the producers on the ground. This allowed collaboration and consultation about how a shot might be reframed or exposure adjusted, or discussion on suggestions for previously unplanned shots.</p>\n<p/>\n<p>However, the moviemaking secret of <em>A Beautiful Planet</em> is that the astronaut filmmakers actually took very little video of the Earth. It was realized that the digital video cameras currently available don\u2019t have the resolution needed for the enormous IMAX screen. Instead, says <a shape='rect' href='http://www.imdb.com/name/nm0624648/'>James Neihouse</a>, the film\u2019s director of photography, still digital cameras such as the <a shape='rect' href='https://www.usa.canon.com/internet/portal/us/home/products/details/cameras/cinema eos/eos c500'>Canon EOS C500 EF</a> were used. These captured 4K images at four frames per second in daylight, and two frames per second in the Earth\u2019s shadow. About 250,000 still frames\u2014totaling about 11 terabytes of raw data\u2014were captured in this way. Back on Earth, software was used to interpolate additional in between frames to create video files with smooth motion. The result was roughly 18 terabytes of footage that had to be edited down for the finished movie.</p>\n<p/>\n<p>Opting for digital cameras over film also allowed images to be captured with very high dynamic ranges, so that dimmer features like stars or the aurorae were not washed out by bright objects in the foreground. The result is spectacular scenes of the sort that would have been previously impossible to see on a screen without resorting to post production editing wizardry like compositing separately shot footage together or using computer generated graphics.</p>\n<p/>\n<p>The astronauts who captured the footage worked in their spare time outside their official scientific and engineering jobs. Neihouse led the team that got the astronauts up to speed on handling the cameras. He notes that the training involved breaking the astronauts of some of the videography habits that NASA had trained into them, such as ensuring that everyone in a shot was oriented with their feet pointing the right direction. This \u201c1 g mode\u201d is good for a press conference beamed from space, but doesn\u2019t convey the true topsy turvy nature of life onboard the station. Neihouse laughs that he\u2019s \u201cthe only [director of photography] in the world who has to train their first unit how to shoot.\u201d </p>\n</div>\n</body>\n</html>","journal":"ieee","category":"aerospace","Topic 1":0.00534431043869145,"Topic 2":0.033328844772525,"Topic 3":0.00500444993293807,"Topic 4":0.049906844838778,"Topic 5":0.00348854110240497,"Topic 6":0.00551896840280702,"Topic 7":0.0287729238096297,"Topic 8":0.00354407292895701,"Topic 9":0.00551666676252055,"Topic 10":0.00623270283486204,"Topic 11":0.000563005706724661,"Topic 12":0.00118141560355834,"Topic 13":0.00281334310619709,"Topic 14":0.00377180554695563,"Topic 15":0.000911197447337996,"Topic 16":0.00434861676631852,"Topic 17":0.0143752821570796,"Topic 18":0.006530193452362,"Topic 19":0.00693017701506755,"Topic 20":0.00606595583663483,"Topic 21":0.0210479306503121,"Topic 22":0.0281699173979949,"Topic 23":0.00503190723317508,"Topic 24":0.00417333889699441,"Topic 25":0.0108063347510849,"Topic 26":0.00406177150451672,"Topic 27":0.00962725049118646,"Topic 28":0.0141675093673966,"Topic 29":0.0109953056146806,"Topic 30":0.00378148966377342,"Topic 31":0.00956881200678612,"Topic 32":0.0142332663946875,"Topic 33":0.0181281760721104,"Topic 34":0.0193110749489662,"Topic 35":0.161747577051206,"Topic 36":0.255444921852253,"Topic 37":0.0164275661320264,"Topic 38":0.018272616396718,"Topic 39":0.0642952098502199,"Topic 40":0.00130752755570227,"Topic 41":0.0673123040903472,"Topic 42":0.003130164728291,"Topic 43":0.0448087088872206},{"id":2,"body":"<?xml version='1.0' encoding='UTF 8'?><html>\n<body>R&amp;D lab Draper is using genetic engineering and optoelectronics to build cybernetic insects<figure>\n<img src='http://spectrum.ieee.org/image/Mjg1NzUyMQ.jpeg'/>\n<figcaption>Photo: Draper</figcaption>\n<figcaption>The R&amp;D company Draper is developing an insect control 'backpack' with integrated energy, guidance, and navigation systems, shown here on a to scale dragonfly model.</figcaption>\n</figure>\n<div>\n<p>As hard as we\u2019re trying, it\u2019s going to be a very long time before we\u2019re able to build a robotic insect that\u2019s anywhere near as capable or versatile as a real one. So for now, we rely on a cybernetics approach to get real insects to do our bidding instead. Over the past several years researchers have managed to <a shape='rect' href='http://spectrum.ieee.org/robotics/military robots/cyborg moth gets a new radio'>steer large insects using electrical implants</a>, a sort of brute force method with limited real world usefulness.</p>\n<p>Now engineers at the <span>R&amp;D company </span>Draper, based in Cambridge, Mass., are hoping to overcome those limitations by creating a cybernetic dragonfly that combines \u201cminiaturized navigation, synthetic biology, and neurotechnology.\u201d <span>To steer the dragonflies, the Draper engineers are developing a way of </span>genetically modifying<span> the nervous system of the insects so they can respond to pulses of light. </span>Once they get it to work, this approach, known as optogenetic stimulation, could enable dragonflies to carry payloads or conduct surveillance, or even help honey bees become better pollinators.</p>\n<aside class='inlay pullquote rt med'>\n To steer the dragonflies, the engineers are developing a way of genetically modifying the nervous system of the insects so they can respond to pulses of light. \n</aside>\n<p>The DragonflEye project is a collaboration between Draper and the <a shape='rect' href='https://www.janelia.org/'>Howard Hughes Medical Institute (HHMI) at Janelia Farm</a>. There are several unique technologies that have been implemented here: The group was able to pack all of the electronics into a tiny \u201cbackpack,\u201d meaning that small insects (like bees and dragonflies as opposed to large beetles) can fly while wearing it. Some of the size reduction comes from the use of solar panels to harvest energy, minimizing the need for batteries. There\u2019s also integrated guidance and navigation systems, so a fully autonomous navigation is possible outside of a controlled environment.</p>\n<p>Another major advance is that, rather than using electrodes to brute force the muscles of an insect into doing what you want, the Draper engineers are taking a more delicate approach, using what are called optrodes to activate a special type of \u201csteering\u201d neuron with light pulses. These steering neurons act as a bridge between the dragonfly\u2019s sensors and its muscles, meaning that accessing them provides a much more reliable form of control over how the insect moves.</p>\n<p>For more details, we spoke with Jesse J. Wheeler, a senior biomedical engineer at Draper and the principal investigator on the DragonflEye program.</p>\n<p>\n<strong>\n<em>IEEE Spectrum</em>: How is your work different from (or related to) some of the cybernetic insects that have been presented in the past?</strong>\n</p>\n<p>\n<a shape='rect' href='http://www.cell.com/current biology/abstract/S0960 9822(15)00083 4'>Previous</a>\n<a shape='rect' href='https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2821177/'>attempts</a> to guide insect flight used larger organisms like beetles and locusts so that they could lift relatively large electronics systems that weighed up to 1.3 grams. These systems did not include navigation systems and required wireless commands to guide flight. Two approaches were attempted: Spoofing sensory inputs to trigger flight behaviors, and directly stimulating the neurons and muscles that control the wings. The challenge with spoofing sensory inputs is that organisms often adapt and learn to ignore the sensory information that isn\u2019t consistent with other senses. The challenge with directly controlling wings is that it degrades the insect\u2019s inherently elegant neuromuscular control required for sustained stable flight. These systems also used electrical stimulation, which is imprecise and indiscriminately activates any neurons or muscles near to the electrodes.</p>\n<p/>\n<figure role='img' class='rt med lrg'>\n<img src='http://spectrum.ieee.org/image/Mjg1NzcxNw.jpeg' alt='Draper Cyborg Dragonfly'/>\n<figcaption class='hi cap'>\n  Photo: Draper\n </figcaption>\n<figcaption>\n  Close up of the electronic control \u201cbackpack\u201d before it gets folded and fitted to a dragonfly.\n </figcaption>\n</figure>\n<p>Our approach is different because we are using dragonflies, which are smaller and more agile fliers. The DragonflEye backpack is designed to navigate autonomously without wireless control, harvest energy from the environment for extended operation, and is a fraction of the weight for smaller insects. Research by our collaborator, Anthony Leonardo, Janelia Research Campus group leader, has studied how special \u201csteering\u201d neurons in the dragonfly control flight direction. These special steering neurons are a type of interneuron, which is neither sensory nor motor. These interneurons are believed to provide steering commands to downstream neuromuscular circuits that coordinate muscle control of the wings and maintain stable flight. These steering neurons will be precisely activated without inadvertently activating nearby neurons and muscle through optogenetic stimulation. This approach will allow us to activate individual neurons with light, which can\u2019t be done with electricity.</p>\n<p>\n<strong>How do optrodes work, and what are the advantages of using them to interface with neurons?</strong>\n</p>\n<p>Much like an electrode, which creates an electrical interface with neurons, an optrode creates an optical interface, allowing light to be either delivered to neurons for stimulation or be captured from light emitting neurons for monitoring activity. While neurons in the retina are naturally activated by light, which allows you to see, neurons in the rest of the body are not naturally sensitive to light. By inserting genetic material that encodes special light sensitive proteins, called opsins, neurons can be modified to be activated, or even inhibited, by different colors of light.</p>\n<p>In addition, genetic material can be inserted that causes neurons to emit light when they are active. These new optogenetic tools allow optrodes to both monitor and stimulate neurons with far greater specificity than can be achieved by electrodes. A reason for this improved specificity is that electrical fields interact with all neurons in proximity to the electrode, but light will only interact with neurons that have been genetically modified. Additionally, while electric fields are good at activating neurons, it is more difficult to inhibit them. In contrast, different types of opsins can be used to both activate and inhibit neurons simply by changing the color of light passed through the optrode.</p>\n<p>\n<strong>Can you describe the components and capabilities of the backpack guidance system in more detail, and why did you decide to mount it on a dragonfly specifically?</strong>\n</p>\n<p>The backpack is designed to navigate autonomously, harvest solar energy for extended operation, deliver light pulses through optrodes to control steering neurons, and wirelessly transmit data to an external base station. This is our first generation system, which will allow us to develop insect guidance using optogenetic stimulation. Next steps will further reduce the size and weight of the DragonflEye system by developing a custom integrated system on chip. Further miniaturization will reduce the payload burden and allow the system to be worn by even smaller insects. </p>\n<p>Dragonflies are interesting because they are found worldwide and are very robust and agile fliers for their small size. Future work could extend guidance to other insects, including important pollinators.</p>\n<p>\n<strong>Why is a cybernetic insect a good idea as opposed to attempting to develop an insect size flying robot?</strong>\n</p>\n<p>Common dragonflies weigh around 600 milligrams, can reach accelerations up to 9 <em>g</em>s, and are known to migrate over great distances. Mechanical fliers of comparable size are far less efficient at producing lift, stabilizing flight, and storing energy. This inefficiency creates a fundamental challenge: Mechanical fliers can carry only very small power sources, which means that they have enough power to fly for only very brief periods of time. The DragonflEye system doesn\u2019t require a power source for flight, only for navigation. It can operate indefinitely due to the insect\u2019s ability to replenish energy from food and the navigation system\u2019s ability to harvest energy from the environment.</p>\n<aside class='inlay pullquote xlrg'>\n \u201cThe DragonflEye system offers new miniaturized technology to equip a wide range of insects with environmental sensors and potentially guide important behaviors, like pollination.\u201d\n <span class='pq attrib'>\u2014Jesse J. Wheeler, Draper</span>\n</aside>\n<p>\n<strong>Can you expand a bit on your vision for applications for robots like these?</strong>\n</p>\n<p>Tracking insects and small animals will enable researchers to better understand behavior in the wild, monitor the influence of environmental changes, and help to guide policies to protect important ecosystems. Beyond tracking, the DragonflEye system offers new miniaturized technology to equip a wide range of insects with environmental sensors and potentially guide important behaviors, like pollination. </p>\n<p>\n<strong>What is the current state of this project?</strong>\n</p>\n<p>In order to begin guidance of dragonflies, several key technologies needed to be developed. [HHMI] focused on developing gene delivery methods specific to the dragonfly to make special steering neurons sensitive to light. Draper developed a miniaturized backpack for autonomous navigation and a flexible optrode to control the modified neurons by guiding light around the dragonfly\u2019s tiny nerve cord.</p>\n<p>Our first generation system is based upon early mockup backpacks that were fitted to dragonflies to test ergonomics and weight limits. With these new technologies in hand, we will be equipping dragonflies with the backpack system and begin investigating position tracking, flight control, and optimized optical stimulation.</p>\n<p>\n<strong>What are you working on next?</strong>\n</p>\n<p>In the first year of the project, we focused on developing core enabling technologies like the backpack, optrode, and synthetic biology tool kit for the dragonfly. As we begin our second year, we are preparing to equip dragonflies with our first generation backpacks in a motion capture room that can monitor their precise flight movements as data is captured from navigation system. This will allow us to develop precise onboard tracking algorithms for autonomous navigation.</p>\n<p>Next, we will apply optical stimulation from the backpack to trigger flight behaviors, which will allow us to develop autonomous flight control. In parallel, we are working on our second generation backpack, which will simultaneously increase functionality while significantly reducing weight and size.</p>\n<p>[ <a shape='rect' href='http://www.draper.com/news/equipping insects special service'>Draper</a> ]</p>\n</div>\n</body>\n</html>","journal":"ieee","category":"automaton","Topic 1":0.000269471060129218,"Topic 2":0.0039191488229209,"Topic 3":0.00159784633738111,"Topic 4":0.000583569815570579,"Topic 5":0.122465232938289,"Topic 6":0.00176614887904846,"Topic 7":0.00286549565226542,"Topic 8":0.00281064180507187,"Topic 9":0.00821788735091918,"Topic 10":0.000561653955047338,"Topic 11":0.00115939779348684,"Topic 12":0.000203038005488994,"Topic 13":0.00108614697457603,"Topic 14":0.00212424453491656,"Topic 15":0.000390131575911173,"Topic 16":0.0242143683553396,"Topic 17":0.000465954386981008,"Topic 18":0.130123297867686,"Topic 19":0.226677494021378,"Topic 20":0.0101471614950389,"Topic 21":0.0119182972721949,"Topic 22":0.00369905261674589,"Topic 23":0.00124730306682263,"Topic 24":0.000686110344817331,"Topic 25":0.000783599402565843,"Topic 26":0.000145958746170182,"Topic 27":0.0621971540491028,"Topic 28":0.00173600881251673,"Topic 29":0.0247323886463811,"Topic 30":0.00146147477930758,"Topic 31":0.214452885063228,"Topic 32":0.0014820752601091,"Topic 33":0.00199412711351421,"Topic 34":0.002693318536796,"Topic 35":0.00221272181671863,"Topic 36":0.00858726355070389,"Topic 37":0.00969823761692584,"Topic 38":0.00330825011339156,"Topic 39":0.00176675811033992,"Topic 40":0.035401202827168,"Topic 41":0.00710726941995067,"Topic 42":0.0595127331621387,"Topic 43":0.00152747804494382},{"id":3,"body":"<?xml version='1.0' encoding='UTF 8'?><html>\n<body>Controlling the electrical current for heating graphene allows sound frequencies to be mixed together, amplified, and equalized<figure>\n<img src='http://spectrum.ieee.org/image/Mjg5NTYyNw.jpeg'/>\n<figcaption>Photo: David Horsell/University of Exeter</figcaption>\n</figure>\n<div>\n<p>The history of using nanomaterials such as <a shape='rect' href='http://spectrum.ieee.org/nanoclast/consumer electronics/audiovideo/nanoparticles enable novel loudspeaker design'>magnetic nanoparticles</a> or <a shape='rect' href='http://spectrum.ieee.org/nanoclast/semiconductors/materials/carbon nanotube speakers promise applications outside of audio equipment'>carbon nanotubes</a> in audio speakers has mainly been to demonstrate the capabilities of these materials rather than to yield speakers that will actually be listened to. That changed last year when <a shape='rect' href='http://spectrum.ieee.org/nanoclast/consumer electronics/gadgets/graphene enables flat speakers for mobile audio systems'>South Korean researchers used graphene to produce a speaker that does not require an acoustic box to produce sound</a>.</p>\n<p/>\n<p>Now researchers from the University of Exeter in the UK are turning again to graphene to make a speaker that produces sound thermoacoustically. Instead of depending on vibrations of a material inside of an acoustic box, thermoacoustics leverages a century old idea that sound can be produced by the rapid heating and cooling of a material. Where the Exeter researchers\u2019 work departs from that of their Korean counterparts is that this newest device serves not only as a speaker, but also as an amplifier and graphic equalizer\u2014all on a thumbnail size chip.</p>\n<p/>\n<p>In research described in the journal <a shape='rect' href='https://www.nature.com/articles/s41598 017 01467 z'>\n<em>Scientific Reports</em>\n</a>, the Exeter scientists were able to demonstrate that when graphene is rapidly heated and cooled by an alternating electric current, it transfers those thermal variations to the surrounding air. The air expands and contracts, thereby generating sound waves. The key to the device\u2019s multipurpose capability is controlling the electrical current going into the graphene.</p>\n<p/>\n<p>\u201cThermoacoustics (conversion of heat into sound) has been overlooked because it is regarded as such an inefficient process that it has no practical applications,\u201d explained David Horsell, a senior lecturer at Exeter and lead author of the paper, in a press release. \u201cWe looked instead at the way the sound is actually produced and found that by controlling the electrical current through the graphene we could not only produce sound but could change its volume and specify how each frequency component is amplified. Such amplification and control opens up a range of real world applications we had not envisaged.\u201d</p>\n<p/>\n<p/>\n<p>While mobile devices are the first applications to come to mind for such a device, other uses are being considered. Because graphene is almost completely transparent, the researchers envision that this technology could be used to transmit both pictures and sound. Because of this capability, the researchers believe that it could be used for <a shape='rect' href='http://spectrum.ieee.org/searchContent?q=ultrasound+imaging&amp;type=&amp;sortby=newest'>ultrasound imaging</a> in medical applications.</p>\n<p/>\n<p>Based on graphene\u2019s strength and flexibility, the Exeter scientists believe that an ultrasound device enabled with this graphene chip could offer better imaging because of better contact with the patient. Also, because the device is relatively cheap to produce, the Exeter team believes that it could someday be used in <a shape='rect' href='http://spectrum.ieee.org/searchContent?q=real time+patient+monitoring&amp;type=&amp;sortby=relevance'>real time patient monitoring</a> applications such as intelligent bandages.</p>\n<p/>\n<p>Until these more speculative applications come to fruition, Horsell sees a real and immediate use for the technology in the telecommunications industry. Noting the technology\u2019s frequency mixing capability, Horsell added, \u201cThe most exciting thing is that it does this trick of multiplication in a remarkably simple and controllable way. This could have a real impact in the telecommunications industry, which needs to combine signals this way but currently uses rather complex and, therefore, costly methods to do so.\u201d</p>\n</div>\n</body>\n</html>","journal":"ieee","category":"nano","Topic 1":0.000589889034744178,"Topic 2":0.000785931103182742,"Topic 3":0.00222858307642994,"Topic 4":0.000671337276901781,"Topic 5":0.00298937689378423,"Topic 6":0.000674702129695975,"Topic 7":0.0519671280380552,"Topic 8":0.00125083980369775,"Topic 9":0.0121397096231406,"Topic 10":0.00148256003803869,"Topic 11":0.022234772269883,"Topic 12":0.000477656932764984,"Topic 13":0.0004520410340397,"Topic 14":0.000955588660955621,"Topic 15":0.000338574006871056,"Topic 16":0.0169226367001672,"Topic 17":0.000294911563289214,"Topic 18":0.00100136715640881,"Topic 19":0.196538606918556,"Topic 20":0.00561784553066322,"Topic 21":0.0303913922739501,"Topic 22":0.0024597199890939,"Topic 23":0.00186288247057067,"Topic 24":0.000437769090774382,"Topic 25":0.0198830700108552,"Topic 26":0.000419955685060578,"Topic 27":0.516348300339418,"Topic 28":0.00285355293591752,"Topic 29":0.00528592932569323,"Topic 30":0.00151701410339876,"Topic 31":0.000354691804869397,"Topic 32":0.00127825992666326,"Topic 33":0.000211636463896395,"Topic 34":0.0035474090213586,"Topic 35":0.00203272099064404,"Topic 36":0.0279846015572522,"Topic 37":0.000850549884279424,"Topic 38":0.00490804236156976,"Topic 39":0.00290190568590971,"Topic 40":0.0210984505670953,"Topic 41":0.0254014260773447,"Topic 42":0.00767457357164637,"Topic 43":0.000682088071469221},{"id":4,"body":"<?xml version='1.0' encoding='UTF 8'?><html>\n<body>AI powered traffic light coordination would cut time spent in the car, traffic congestion, and emissions<figure>\n<img src='http://spectrum.ieee.org/image/MjgyMDk1MQ.jpeg'/>\n<figcaption>Photo: iStockphoto</figcaption>\n</figure>\n<div>\n<p>Idling in rush hour traffic can be mind numbing. It also carries other costs. Traffic congestion costs the U.S. economy $121 billion a year, mostly due to lost productivity, and produces about 25 billion kilograms of carbon dioxide emissions, Carnegie Mellon University professor of robotics Stephen Smith told the audience at a <a shape='rect' href='http://www.frontiersconference.org'>White House Frontiers Conference</a> last week. In urban areas, drivers spend 40 percent of their time idling in traffic, he added.</p>\n<p/>\n<p>The big reason is that today\u2019s traffic signals are dumb. Smith is developing smart <a shape='rect' href='http://spectrum.ieee.org/robotics/artificial intelligence'>artificial intelligence</a> fueled traffic signals that adapt to changing traffic conditions on the fly. His startup <a shape='rect' href='https://www.surtrac.net'>Surtrac</a> is commercializing the technology.</p>\n<p/>\n<p>In pilot tests in Pittsburgh, the smart traffic management system has gotten impressive results. It reduced travel time by 25 percent and idling time by over 40 percent. That means less time spent staring out the windshield and more time working, being with your family, or doing anything else. I\u2019m a Pittsburgh resident who has witnessed the city\u2019s rapidly evolving urban landscape. And I can attest to the mostly frustration free driving that has resulted from this system despite the city\u2019s growing population. </p>\n<p>The researchers also estimate that the system cuts emissions by 21 percent. <span>It could also save cities the cost of road widening or eliminating street parking by boosting traffic throughput.</span>\n</p>\n<p/>\n<p>Conventional traffic lights have preprogrammed timing that\u2019s updated every few years. But as traffic patterns evolve, the systems can fall out of date much more quickly than that.</p>\n<p/>\n<p>The Surtrac system instead relies on computerized traffic lights coordinating closely with each other. Radar sensors and cameras at each light detect traffic. Sophisticated AI algorithms use that data to build a timing plan \u201cthat moves all the vehicles it knows about through the intersection in the most efficient way possible,\u201d Smith says. The computer also sends the data to traffic intersections downstream so they can plan ahead.</p>\n<p/>\n<p>Unlike other <a shape='rect' href='http://spectrum.ieee.org/geek life/history/the man who invented intelligent traffic control a century too early'>smart traffic management systems</a>, such as one used in Los Angeles, Smith emphasized that this one is decentralized. So each signal makes its own timing decisions, making it a truly smart system.</p>\n<p/>\n<p>Smith\u2019s team started by implementing the AI traffic control system at nine intersections in Pittsburgh\u2019s busy East Liberty neighborhood in 2012. The network now spans 50 intersections, with plans to expand it citywide. </p>\n<p/>\n<p>The next step is to have traffic signals talk to cars. Smith\u2019s group has already installed <a shape='rect' href='http://spectrum.ieee.org/cars that think/transportation/infrastructure/cars talk to cars on the autobahn'>short range radios</a> at 24 intersections. Such systems are expected to begin being built into some cars in 2017, he said. Traffic signals could then let drivers know of upcoming traffic conditions or let them know that lights are about to change, increasing safety and relieving congestion.</p>\n<p/>\n<p>A <a shape='rect' href='http://spectrum.ieee.org/cars that think/transportation/advanced cars/researchers prove connected cars can be tracked'>vehicle to infrastructure communication system</a> could also prioritize certain vehicles. The CMU team is working with the Pittsburgh Port Authority to develop a system that prioritizes public transport buses.</p>\n<p/>\n<p>Pittsburgh is the <a shape='rect' href='http://spectrum.ieee.org/cars that think/transportation/self driving/uber will start driverless service in pittsburghthis month'>test bed for Uber\u2019s self driving cars</a>, and Smith\u2019s work on AI enhanced traffic signals that talk with self driving cars is paving the way for the ultimately <a shape='rect' href='http://spectrum.ieee.org/cars that think/transportation/self driving/the scary efficiency of autonomous intersections'>fluid and efficient autonomous intersections</a>.</p>\n<p/>\n<p/>\n</div>\n</body>\n</html>","journal":"ieee","category":"cars","Topic 1":0.0078503308192551,"Topic 2":0.00197366589948203,"Topic 3":0.00564347181153215,"Topic 4":0.00266491058395464,"Topic 5":0.0016871142820779,"Topic 6":0.165237133768256,"Topic 7":0.0143422064118793,"Topic 8":0.0219475730714653,"Topic 9":0.00259525628066222,"Topic 10":0.00429026230414995,"Topic 11":0.000585705623399895,"Topic 12":0.000794805801011266,"Topic 13":0.0154227193805041,"Topic 14":0.00741274641688968,"Topic 15":0.00287139380435988,"Topic 16":0.00303522708775907,"Topic 17":0.00246365388265055,"Topic 18":0.0192611757190405,"Topic 19":0.0212937110542103,"Topic 20":0.00302499434071791,"Topic 21":0.0351131956180682,"Topic 22":0.000776615132479404,"Topic 23":0.00282929559207995,"Topic 24":0.00211003601584748,"Topic 25":0.00336903212992642,"Topic 26":0.0286770133015562,"Topic 27":0.00267830394888456,"Topic 28":0.00618084527450095,"Topic 29":0.00363372000858842,"Topic 30":0.00186634925850121,"Topic 31":0.00524563199466585,"Topic 32":0.025065873961915,"Topic 33":0.00331515170690811,"Topic 34":0.00461665191255476,"Topic 35":0.00195666127616969,"Topic 36":0.00440636667426764,"Topic 37":0.375142544185235,"Topic 38":0.00114981349680068,"Topic 39":0.016332955875091,"Topic 40":0.124995438454713,"Topic 41":0.0093425476320337,"Topic 42":0.0353965478553793,"Topic 43":0.00140135035057585}]
